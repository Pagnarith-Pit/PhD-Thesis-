# Motivation
Beyond their application as a tutor, since the release of large-scale generative AI models, the higher education sector has been repeatedly identified as one of the domains poised to benefit most profoundly from these technologies. Academic literature has highlighted the potential of generative AI to expand access to personalised feedback, enhance student support, reduce administrative burdens, and provide scalable tutoring capabilities across diverse disciplines. As a result, universities worldwide have faced strong calls from researchers, policymakers, and industry to integrate AI into teaching and learning environments in meaningful ways.

Yet, despite these optimistic projections, a growing counter-movement has emerged, warning against uncritical or extensive adoption of AI in education. Critics argue that generative AI may foster over-reliance, enable academic misconduct, disrupt the development of critical thinking, and reduce opportunities for meaningful peer interaction and engagement within classrooms. Some educators also express concerns about cognitive offloading, arguing that if AI tools are used to bypass effortful thinking—such as solving exercises, generating explanations, or producing writing—students may experience long-term erosion in analytical and problem-solving skills. These opposing narratives have created a polarised discourse in which AI is simultaneously framed as a transformative educational innovation and a potentially harmful shortcut.

If one side of this debate is to ultimately prevail, the true benefits and harms of AI must be empirically established rather than assumed. Surprisingly, however, large-scale institutional adoption remains limited. This is particularly puzzling given that a broad body of non-experimental research—primarily surveys and interviews conducted across diverse geographical regions—reports overwhelmingly positive attitudes toward AI among university students. Students consistently describe AI tools as convenient, accessible, and beneficial for understanding complex content, clarifying concepts, and supporting their studies. These findings might lead one to expect rapid and widespread adoption of AI-driven educational practices.

Instead, studies involving real student usage paint a much more complex and inconsistent picture. In controlled or course-embedded deployments, some disciplines—most notably computer science and programming—report high student engagement with AI assistants and clear preferences for studying with AI support. Students in these fields often describe AI as a “debugging companion,” a “coding partner,” or a “24/7 tutor,” suggesting that AI meaningfully enhances their learning experience. Conversely, other disciplines show low uptake or limited interest in using AI for learning tasks, even when students are familiar with the technology. In some humanities and social science contexts, for example, students express concerns about the accuracy, relevance, or disciplinary appropriateness of AI-generated outputs, leading to cautious or minimal adoption.

Complicating matters further, several recent studies have adopted a more naturalistic, observational approach by analysing students’ real chat logs with AI systems. Unlike controlled experiments, these studies do not restrict the types of questions students can ask or the timeframe of usage. Instead, they capture authentic, everyday interactions. Findings from such analyses reveal that a substantial portion of student queries are not aligned with deep learning goals. Many interactions involve asking AI for direct answers to homework questions, generating assignment-ready content, or using AI for convenience-based tasks rather than meaningful cognitive engagement. Other interactions are unrelated to academic work altogether, involving personal, professional, or entertainment-oriented queries. These observations challenge the assumption that enthusiastic attitudes towards AI necessarily translate into beneficial or pedagogically sound usage patterns.

Together, these contradictions raise a critical question about where the actual educational value of generative AI lies. If student attitudes are overwhelmingly positive, yet real-world usage varies widely and often gravitates toward shallow or non-academic activities, then institutions cannot assume that AI will automatically enhance learning. Understanding the real value of AI is essential not only for justifying its incorporation into everyday university learning environments but also for guiding strategic institutional investment. Moreover, insights into how students genuinely use AI are vital for developers of educational AI systems, who must align their tools with what meaningfully benefits students rather than what is simply convenient or superficially appealing.

These concerns motivate the central problem addressed in this research: although generative AI has rapidly become ubiquitous, its actual impact on students’ learning practices remains unclear. Without a grounded, empirical understanding of how AI reshapes the everyday routines, strategies, and behaviours of university students, the sector risks designing policies, technologies, and teaching practices based on assumptions rather than realities.

## Research Question
Thus, the key research question guiding this study is:

How does the use of generative AI reshape students’ learning practices?

To examine this question, the study further investigates the following subquestions:

1) In what ways do students integrate AI tools with university-provided learning materials (such as textbooks, lecture notes, and LMS content)?

2) What types of learning activities or outcomes are enabled by AI tools, and which are simply eased—made more efficient or convenient—without fundamentally transforming the underlying learning process?

Through addressing these questions, this research aims to illuminate the real, rather than presumed, role of generative AI in contemporary university education and to provide evidence-based guidance for institutions, educators, and developers navigating this rapidly changing landscape.

# Preliminary Study
Here, investigate the adoption of AI tutor at the University of Melbourne. Talk about our effort in first year attempt at COMP90059. Describe

### Study Design
recruitment strategy - subject coordinator endorsement, training video, personal introduction at tutorials and lectures, tutors reminders
target population
compensation - none, volunteer basis, annonymised data

### Technical Report
Web interface
Web URL access link and hosting infrastructure
Weekly tutorial embedding
Chat area

### AI features
Socratic tutor
Progressive hints

### Results
Low adoption, only 10 sign ups, 162 collected conversation threads, averaged around 3 conversation lengths
confirmed through chatlogs- reasons for dropoffs include requesting for answers or move on request, denoting frictions 

# Methods

This study adopts a mixed-methods Exploratory Sequential design, integrating qualitative and quantitative research to develop a comprehensive understanding of how university students incorporate generative AI tools into their learning practices. The decision to employ an exploratory design reflects the emerging and understudied nature of day-to-day AI use in higher education. While substantial policy discussions and conceptual debates surround AI in learning, empirical evidence capturing how students actually use AI remains limited. Combining qualitative exploration with quantitative validation therefore enables both openness to new patterns and the ability to evaluate their generalisability across a larger population.

### Theoretical Framework

Past research on AI in education has predominantly relied on frameworks such as:

* Technology Acceptance Model (TAM)

* Self-Regulated Learning (SRL)

* Unified Theory of Acceptance and Use of Technology (UTAUT)

* Technology–Task Fit (TTF)

These models have been valuable for assessing perceived usefulness, ease of use, and self-efficacy, yet they offer limited insight into the specific sources of value students derive from AI. In other words, these frameworks explain whether students find AI helpful, but not how or why it changes their learning.

To address this gap, the present study draws on the Substitution, Augmentation, Modification, and Redefinition (SAMR) framework. SAMR provides a structured way to distinguish between:

* Easing uses (defined in original study as enhacement): Sub and Augment, where AI substitutes or augements tasks without changing or little change to learning practices, and

* Enabling uses (defined in original study as transforming) : Mod and Redefine, where AI introduces fundamentally new learning opportunities or transforms how tasks are approached.

SAMR is therefore uniquely suited to evaluating whether AI enhances education in a pedagogically meaningful way or merely accelerates or outsources existing tasks. This distinction is especially relevant given current debates on academic integrity, cognitive offloading, and the risk of over-reliance on AI.

We explicitly use SAMR as "sensitising framework". A sensitising concept helps us notice relevant things without determining what the data must be. This means that SAMR is used to guide question creation, but then analysis are performed inductively without forcing data into a predefined model. After themes emerge, we then apply SAMR deductively for final interpreation. Although the SAMR framework is widely used to categorise technology-enhanced learning tasks, it was not designed as a psychometric model for measuring student behaviour or attitudes. Directly operationalising SAMR levels as study constructs risks conflating task-level pedagogical classification with learner-level orientation. Accordingly, this study measures established constructs related to technology acceptance, learning orientation, and cognitive engagement, and uses the SAMR framework as an interpretive lens to analyse patterns of AI use rather than as a direct measurement instrument.

The original SAMR by Puentedura (2009, 2020), however, is too narrow and leaves too much room for interpretation, making it hard to accurately code usages to each level. Instead, this study will use (Crompton & Burke, 2020) extended definition for clear differentiation of each level:

<img width="668" height="595" alt="Screenshot 2025-12-15 at 4 10 31 pm" src="https://github.com/user-attachments/assets/8b94a4aa-9f3b-4e34-9e26-4878151177ab" />


### Justification for Exploratory Design 

Mixed-methods research has been widely adopted in educational technology studies when the aim is to understand both the complexity of human behaviours and the general prevalence of patterns (Creswell & Plano Clark, 2017). Because generative AI is a novel and rapidly evolving tool, student usage has not yet stabilised into predictable or well-defined patterns. Consequently, qualitative inquiry is essential to uncover behaviours, use cases, and perceptions that existing theories or surveys might overlook.

The two phases of this study are designed to function as an integrated exploratory–confirmatory sequence, where qualitative insights from Phase 1 directly inform and structure the quantitative investigation in Phase 2. Phase 1 generates conceptual categories of AI usage through semi-structured interviews, allowing students to describe their behaviours, strategies, and perceptions in their own words. These emergent themes are interpreted through the SAMR framework, which sensitises the analysis to distinctions between uses that enable new learning practices (Modification/Redefinition) and those that ease existing tasks (Substitution/Augmentation). This approach ensures that the qualitative exploration remains inductive, capturing the full range of student experiences, while also producing theoretically meaningful categories that can guide further measurement.

Phase 2 translates these Phase 1 themes into quantifiable constructs within a large-scale survey. Each usage category identified qualitatively becomes a survey item or cluster of items, allowing measurement of its prevalence, frequency, and perceived impact across a broad and diverse student population. SAMR-aligned enabling and easing distinctions inform the coding of these survey responses, providing a consistent lens for interpreting which behaviours represent pedagogically significant transformation versus efficiency-focused convenience. By operationalising qualitative findings in this way, Phase 2 serves as a validation and generalisation step, testing whether patterns observed in the exploratory sample are evident at scale.

Finally, qualitative and quantitative results are integrated through a joint inferential framework. Convergence is assessed where Phase 2 data support Phase 1 themes, demonstrating stability and generalisability of usage patterns. Complementarity is established where qualitative insights explain the mechanisms behind quantitative trends—for example, how specific enabling uses contribute to self-reported understanding or engagement. Divergence is examined where Phase 2 outcomes differ from Phase 1 expectations, highlighting areas where emergent behaviours may not translate into measurable learning outcomes. Together, this analytical integration ensures coherence across the study, linking theory, empirical observation, and interpretation, and providing a comprehensive understanding of how generative AI reshapes students’ learning practices in higher education.

In the section below, we provide the design of each phase of study in more detail. 

## Qualitative Study (Phase 1)
The initial qualitative phase aims to identify the actual ways students integrate generative AI into their everyday coursework. The objective is to generate categories of usage, explore their underlying motivations, and understand whether students experience these practices as transformative or merely convenient.

### Study Design
Data will be collected through semi-structured interviews, each lasting approximately 20 minutes. Semi-structured interviews are chosen because they balance flexibility—allowing students to describe their usage in their own terms—with comparability across participants. This format is particularly appropriate for exploratory technology research, where unknown behaviours may emerge during conversation.

#### Instruments

A custom interview guide will be developed, guided by past SAMR work. Topics will include:

* Concrete examples of recent AI usage within coursework

* Interactions between AI tools and university-provided materials

* Students’ perceived benefits, challenges, and risks

* Perceived changes in study behaviour since adopting AI

* Perceived necessity of AI for certain tasks

The interview guide will be piloted with 2–3 students to ensure clarity and appropriateness.

#### Additional Data to Collect
To contextualise the results and enable later comparison across subgroups, participants will also provide:

* name and email address: only to report findings or contact for any required changes to data. Not used for any analysis
* Year level: to explore whether senior students use AI differently
* course: to account for known differences between STEM and non-STEM use patterns
* self-rated AI familiarity: account for how well students are familiar with their capabilities 
* english level proficiency: studies have reported AI used for translation. Could be indicator here since University of Melbourne is English speaking instritution

#### Recording and Transcription
Interviews will be audio-recorded with participants’ consent. Transcription will be performed using contemporary speech-to-text models known for high accuracy in educational settings. If consent for recording is not granted, detailed notes will be taken instead.

---

### Participants
Participants will be active undergraduate or postgraduate students at the University of Melbourne. 

**Variables of Interests**:
* Primary Language: English or Others
* Year Level: 1,2,3,4 or above
* Degree Level: Undergrad or Postgrad
* Programming Experience Level: Beginner, Intermediate, or Advanced

**Inclusion Criteria**:
No restrictions will be placed on age, residency status, or academic standing.

1) English Proficiency: Since the interview will be conducted in English, students must have sufficiently proficient English skill to understand the project aim and requirements, not only to provide insightful responses but also for informed consent.

2) Recent AI Usage For Learning: Students must have used AI as part of their programming learning for the past 6 months. This ensures that their experience and consequently their responses remain recent and relevant. 

3) Programming Course Requirement: Participants must be currently enrolled in at least one programming-focused subject. For the purpose of this study, a programming-focused subject is defined as a course whose primary learning outcomes involve learning to write, understand, or debug computer programs, including at least one of the following core areas:

* Programming fundamentals (e.g., variables, control flow, functions, data structures)
* General-purpose programming languages (e.g., Python, Java, C/C++)
* Software development principles (e.g., version control, testing, debugging, modular design)
* Databases and data processing (e.g., SQL, data manipulation, data pipelines), where programming constitutes a core assessed component

The subject must involve substantial hands-on programming tasks or assessments, rather than solely conceptual or theoretical content.

**Exclusion Criteria**: 
1) Specialised or Domain-Specific Programming Subjects: To constrain the scope of the study and reduce heterogeneity in learning contexts, students enrolled exclusively in specialised programming subjects will be excluded.

Specialised programming subjects are defined as courses where programming is primarily taught as a tool for a specific application domain, rather than as a general programming skill. Examples include, but are not limited to:

* Machine learning or artificial intelligence courses
* Programming for business or finance
* Bioinformatics or computational biology
* Domain-specific engineering programming courses

Students enrolled in both general programming subjects and specialised programming subjects may still be included, provided they meet the programming course requirement defined above.

--- 

### Recruitment
Recruitment will occur through:

* Flyers posted around campus

* Announcements made in subjects with the support of coordinators

* Personal networks and referrals

All potential participants will receive information about the study’s aims, interview topics, data retention policies, confidentiality protections, result disemmination strategy, and the ethics approval for this project. Participation will proceed only after informed consent is obtained, with sufficient time to review all documents. Participants will be compensated at a pro-rated rate of $50 per hour.

---

### Data Analysis
Two complementary analytical approaches will be used: sentiment analysis and thematic analysis, followed by a computational check using semantic cluster analysis.

#### Sentiment Analysis

Sentiment analysis provides a preliminary quantitative assessment of the emotional tone of participants’ discussions about AI. This is particularly useful given that self-reported attitudes may be influenced by social desirability or normative pressure in higher-education environments. Automated sentiment scoring offers a consistent and unbiased method for identifying whether students’ experiences are broadly positive, negative, or neutral. It also helps detect patterns of enthusiasm, frustration, or ambivalence that might correlate with specific uses of AI.

##### **Step A: Annotate each response by usage type**

* During coding, assign each participant response to a theme (e.g., “concept explanation,” “debugging,” “assignment outsourcing”) from our thematic analysis.
* Each theme should also have its SAMR label: enabling vs easing.

##### **Step B: Sentiment at the theme level**

* Perform sentiment analysis on each response within a theme.
* Then, **aggregate sentiment per theme for each participant** rather than across the entire interview.

**Example:**

| Participant | Theme                 | SAMR     | Sentiment Score |
| ----------- | --------------------- | -------- | --------------- |
| P1          | Concept Explanation   | Enabling | +0.8            |
| P1          | Assignment Completion | Easing   | -0.1            |
| P2          | Debugging             | Enabling | +0.5            |

This keeps the affective tone tied to **specific AI usage patterns**, which is much more informative for interpretation.

* With this, we can answer questions like:

  * Are enabling uses associated with more positive sentiment than easing uses?
  * Do participants express frustration when using AI for certain tasks?
  * Does sentiment correlate with perceived learning impact?

* For a visual or analytical summary, thinking to create heatmaps:

  * X-axis: Usage type/theme
  * Y-axis: Sentiment score
  * Color: SAMR category (enabling/easing)
This allows quick inspection of which tasks students enjoy or dislike and whether enabling uses are more positively experienced.


#### Thematic Analysis

The main analytic process will involve inductive thematic analysis, chosen because it aligns with the exploratory goal of identifying emerging patterns without forcing responses into predefined categories.

Procedure: 

1) Initial coding

Each interview transcript will be read closely, and segments describing specific AI-related behaviours will be given short descriptive codes (e.g., "summarising lectures", "debugging code", "checking grammar"). Coding will be iterative and evolve as new patterns emerge.

2) Code refinement and grouping

Related codes will be examined and clustered into broader categories or subthemes. These may include categories like “efficiency-driven use,” “task outsourcing,” “conceptual clarification,” or “creativity support.” (Examples only; actual themes will emerge from data.)

3) Theme generation

The subthemes will then be grouped into overarching themes that capture core dimensions of how students apply AI in their study routines.

Because the coding will be conducted by a single primary researcher, the study incorporates an additional computational validity check.


#### Sematic Cluster Analysis (Bias Mitigation)

To reduce potential coder bias and strengthen the reliability of the thematic structure, the study will use semantic cluster analysis:

* Each individual usage example (coded excerpt) will be embedded using a modern text-embedding model.

* A centroid will be computed for each theme.

* Distances between each usage example and all theme centroids will be calculated.

* Instances where the assigned theme is not the nearest centroid will be flagged for review.

This method, commonly used in topic modelling and clustering research, allows a systematic, data-driven check on whether usage descriptions are grouped coherently.

#### Enabling vs. Easing Coding

After identifying themes, each will be evaluated through the lens of the SAMR framework:

* Enabling (Modification/Redefinition):
AI is used in a way that introduces new learning behaviours or capabilities not previously possible. These reflect AI’s potential as a transformative educational technology.

* Easing (Substitution/Augmentation):
AI is used to speed up, automate, or refine existing tasks without fundamentally changing the form of the learning activity. These uses suggest AI may be helpful but not pedagogically significant.

This stage is essential for interpreting not just how students use AI, but whether those uses align with meaningful learning enhancement. It also directly contributes to ongoing debates about whether AI’s benefits outweigh its risks, particularly when uses lean toward convenience or task outsourcing rather than conceptual development.

## Quantitative Study (Phase 2)
Building on the qualitative insights generated in Phase 1, Phase 2 aims to evaluate the generalisability, prevalence, and educational significance of students’ AI usage patterns across a larger and more diverse student population. While Phase 1 identifies how students use AI and why they perceive certain benefits, Phase 2 examines how widespread these behaviours are, how they vary across demographic and academic groups, and whether different forms of AI use (enabling vs. easing) are associated with meaningful learning outcomes. This phase serves to validate the qualitative findings, quantify their distribution, and provide empirical evidence to guide institutional policy and pedagogical decisions.

Phase 2 transforms the rich qualitative insights from Phase 1 into quantifiable, testable constructs, enabling:

* Large-scale validation of usage patterns
* Identification of population-wide trends
* Insights into enabling vs. easing practices
* Evidence-based recommendations for university AI policy
* Clear implications for designing future AI-based educational tools

## **Participants**

The target population mirrors that of Phase 1 but expands its scope to achieve statistical power and representativeness.

### **Who**

Participants will include:

* Undergraduate and graduate students enrolled at the University of Melbourne.
* Students who have used generative AI tools (e.g., ChatGPT, Claude, Gemini, GitHub Copilot) within the past six months, ensuring responses reflect current habits and not outdated behaviours.

No restrictions will be placed on age, residency status, or academic standing.

### **Sample Size**

The study aims to recruit **at least 300 participants**.
This sample size is chosen to:

* Enable stable estimation for multivariate analyses (e.g., regression, cluster analysis).
* Ensure adequate representation of diverse AI usage patterns identified in Phase 1.

### **Recruitment**

Recruitment will occur through multiple channels to ensure broad reach:

* Learning Management System (LMS) announcements with support from course coordinators.
* Flyers posted in student spaces.
* Email invitations via faculty mailing lists where permissible.
* Social media posts within university-affiliated groups.
* Snowball sampling, where participants may share the study link with peers.

Participants will receive detailed information about the study’s aims, data handling procedures, risks, and consent requirements prior to participation.


## **Study Design**

Phase 2 uses a cross-sectional online survey, administered via Google Forms, allowing efficient distribution to a large number of students and enabling data collection within a consistent and structured format.

### **Instrument**

Instrument has been created and can be found in the Questions.txt. We use Bloom Taxonomy and Self Regular Learning as frameworks to adapt questions from them. The cognitive items in this study were operationalized using the Cognitive Process Dimension of the Revised Bloom’s Taxonomy (Anderson & Krathwohl, 2001). This allows for a clear distinction between Lower-Order Cognition (LOC)—characterized by retrieval and comprehension—and Higher-Order Cognition (HOC)—characterized by the analytical and generative use of AI. This cognitive mapping is complemented by a behavioral assessment grounded in Self-Regulated Learning (SRL) theory (Zimmerman, 2000). To quantify these processes, items were adapted from the Metacognitive Self-Regulation subscale of the Motivated Strategies for Learning Questionnaire (MSLQ; Pintrich et al., 1993). While Bloom’s Taxonomy measures the intellectual depth of the tasks performed with AI, the MSLQ-based items capture the metacognitive agency—the planning, monitoring, and reflection—that students exercise during those interactions.

Integrating these frameworks is critical for mapping the survey results to the SAMR framework (Puentedura, 2014). It allows the research to differentiate between AI as a tool for mere Substitution (low cognitive depth and high dependency) versus a tool for pedagogical Redefinition (high-order cognition and high metacognitive agency). By using the MSLQ as the psychometric bridge to SRL theory and Bloom’s Taxonomy as the anchor for cognitive complexity, this study ensures that the resulting SAMR classifications are rooted in established educational psychology rather than subjective observation.

Rationale: This research design is systematically grounded in a dual-framework approach, utilizing Bloom’s Revised Taxonomy and Zimmerman’s Self-Regulated Learning (SRL) model to bridge the gap between cognitive depth and behavioral autonomy. By adapting validated items from the Motivated Strategies for Learning Questionnaire (MSLQ) and Bloom’s cognitive process dimensions, the survey instrument captures not just what tasks students perform with AI, but the metacognitive rigor they apply during the process. To ensure these theoretical constructs are reflected in the actual data, Confirmatory Factor Analysis (CFA) is employed via the semopy library in Python. This stage involves performing factor loading, where we statistically test the relationship between observed survey responses and their underlying latent factors: Lower-Order Cognition (LOC), Higher-Order Cognition (HOC), Metacognitive Agency, and Cognitive Dependency. A high factor loading (typically $>0.6$) confirms that the survey items are robust indicators of their intended constructs.Once these latent factor scores are calculated for each participant, they serve as the empirical "ingredients" for SAMR Mapping. Rather than treating the SAMR (Substitution, Augmentation, Modification, Redefinition) framework as a direct measurement tool, it is used as a post-hoc interpretive lens to classify student profiles. For instance, a profile characterized by high Dependency scores and low Higher-Order Cognition is mapped to Substitution, whereas a profile showing high Metacognitive Agency combined with high-level Bloom tasks (Creating/Evaluating) is classified as Redefinition. This sequential logic—moving from theory-grounded item creation to statistical validation, and finally to pedagogical classification—ensures that the study provides a rigorous, data-driven map of how AI is truly transforming the programming classroom.

### **Data Collected**

The survey will collect multiple categories of data to support rich quantitative analysis:

## **Analysis Plan**

Phase 2 will use a combination of descriptive, inferential, and exploratory statistical analyses:

**Notation:**
  * $n$ = number of participants (target $\ge 300$)
  * $K$ = number of usage themes from Phase 1
  * $E_{ik}$ = participant $i$'s enabling/easing score for theme $k$
  * $E_i = \frac{1}{K} \sum_{k=1}^{K} E_{ik}$ = overall enabling/easing score
  * $U_{ik}$ = usage frequency for theme $k$ (Likert 1–5)
  * $P_i$ = self-rated AI proficiency (1–5)
  * $S_i$ = subgroup membership (STEM/non-STEM, coded 0/1)
  * $Y_i$ = learning outcome variables (perceived understanding, engagement, self-reported performance, etc.)

Missing data will be handled via pairwise deletion or multiple imputation; scales will be standardized as needed $\tilde x = (x-\bar x)/s_x$.

### **1) Descriptive Statistics**

---

### 1.1) Prevalence of each usage pattern

#### **Why This Is Important**

Our qualitative study identified specific usage patterns (e.g., conceptual explanation, summarisation, translation, debugging, assignment completion). The quantitative phase needs to determine:

* Which usage patterns are **common enough** to represent institutional trends
* Which patterns are **emerging behaviours** worth supporting
* Which patterns are **educationally concerning** (e.g., asking AI for answers)

This helps universities and AI tool designers prioritise interventions, support, or guardrails.

#### **How to Interpret the Results**

* **High prevalence** → indicates stable, generalisable behaviours.
* **Moderate prevalence** → suggests context-dependent usage.
* **Low prevalence** → may mean the behaviour is rare, discipline-specific, or emerges only under special circumstances.

If a theme from Phase 1 shows up rarely in Phase 2, we plan to discuss this as:

> “A behaviour observed in qualitative data but not widely generalisable at scale.”

**Inputs:** Usage frequency $U_{ik}$ (1–5)

**Formula:**

* Binary use indicator: $B_{ik} = \mathbf{1}(U_{ik} \ge 2)$
* Count per theme: $n_k = \sum_{i=1}^{n} B_{ik}$
* Prevalence proportion: $\hat p_k = n_k / n$
* 95% CI: $\hat p_k \pm z_{0.975} \sqrt{\hat p_k (1-\hat p_k)/n}$

**Outputs:** Table of counts, proportions, 95% CI; bar charts per theme

### 1.2) Frequency distributions

#### **Why This Is Important**

AI usage behaviours may not be uniformly distributed. Frequency distributions should allow us to observe patterns such as:

* Heavy-tailed usage (a minority use AI extensively)
* Bimodal distributions (groups of avoiders vs. enthusiasts)
* Normal or near-normal patterns (uniform adoption across the student population)

These insights are critical for designing targeted interventions and understanding heterogeneity within the student body.

**Inputs:** $U_{ik}$

**Formula:**
$
\Pr(U_k = r) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}(U_{ik} = r), \quad r \in {1,2,3,4,5}
$

**Outputs:** Histograms, mode, median, IQR for each theme

#### **How to Interpret the Results**

* **Right-skewed** → many light users, few heavy users.
* **Left-skewed** → many heavy users, few light users.
* **Bimodal** → students are polarised; large groups either enthusiastically adopt or avoid AI.
* **Normal distribution** → indicates stable, homogeneous adoption patterns across the population.

This helps the argument of whether AI adoption is **diffuse or polarised**.

### 1.3) Mean scores for enabling vs. easing behaviours

#### **Why This Is Important**

Our thesis will focus on the distinction between:

* **Enabling behaviours** = transformative learning practices
* **Easing behaviours** = efficiency or shortcut-based uses

Mean scores help to quantify *how much* AI is reshaping learning.

#### **How to Interpret the Results**

* **Higher enabling score** → AI is genuinely transforming learning (aligns with SAMR “Modification/Redefinition”).
* **Higher easing score** → AI is primarily used as a substitute or shortcut (aligns with SAMR “Substitution”).
* **Comparable scores** → mixed uses; AI supports both efficiency and transformation.

This connects well to the main research question:

> *How is AI reshaping learning practices?*

**Inputs:** $E_i$ per participant

**Formula:**
$
\bar E = \frac{1}{n} \sum_{i=1}^{n} E_i, \quad SE(\bar E) = s_E / \sqrt{n}
$

where
$
s_E = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (E_i - \bar{E})^2}
$ 

**Outputs:**

* Mean, SD, SE, 95% CI: $\bar E \pm t_{n-1,0.975} SE(\bar E)$
* Boxplots by subgroup (STEM vs non-STEM, year level, etc.)

### **2) Inferential Tests**

---

### 2.1) Comparisons between groups (t-tests)

#### **Why This Is Important**

We'll examine whether particular student groups differ in their AI usage:

* Undergraduates vs. postgraduates
* Language: English vs Others
* High-AI-proficiency vs. low-proficiency users
* Programming Experience Level (Year Level or Course Level)

This helps identify **who benefits most** and where targeted support may be necessary.

#### **How to Interpret the Results**

* **Significant differences** → group membership meaningfully shapes AI use or AI-derived outcomes.
* **Non-significant differences** → behaviours are consistent across demographic or academic groups, suggesting broader generalisability.

Examples:

* If STEM students use enabling AI tools more → this supports the idea that procedural/technical domains benefit differently.
* If international students rely more on translation → this has implications for inclusivity and language support.

**Formula:**
$
t = \frac{\bar E_1 - \bar E_0}{\sqrt{s_1^2/n_1 + s_0^2/n_0}}
$

**Outputs:** t-value, df, p-value, Cohen’s d, 95% CI

### 2.2) Correlations between AI proficiency, usage types, and outcomes

#### **Why This Is Important**

The conceptual model constructed assumes that factors like AI proficiency, usage style, and perceived outcomes are related. Correlations show:

* Whether relationships exist
* Whether these relationships are positive or negative
* How strong they are

#### **How to Interpret the Results**

* **Strong positive correlation** (e.g., enabling use ↗ and perceived learning gain ↗) → enabling AI behaviours are pedagogically beneficial.
* **Strong negative correlation** (e.g., easing use ↗ and engagement ↘) → surface-level use may be detrimental.
* **Weak correlation** → behaviours are independent or context-specific.

Correlations should further support theorisation of *how AI use reshapes learning*.

**Inputs:** $E_i, P_i, U_{ik}, Y_i$

**Formula (Pearson):**
$
r_{XY} = \frac{\sum_i (X_i - \bar X)(Y_i - \bar Y)}{\sqrt{\sum_i (X_i - \bar X)^2 \sum_i (Y_i - \bar Y)^2}}
$

**Outputs:** Correlation matrix, p-values, heatmaps; optionally Spearman for ordinal variables

### **3) Structural and Exploratory Analyses**

---

### **Full Analysis Pipeline: Bloom + Learner Survey → SAMR Interpretation**

---

#### **Step 0: Data Collection**

* Administer the **Bloom + learner-oriented survey** online.
* Collect **demographics & contextual usage** (frequency, tools, task types).
* Optional: include **open-ended questions** to triangulate findings with interviews.

---

#### **Step 1: Data Cleaning & Preparation**

* Remove incomplete responses.
* Reverse-code dependency/offloading items.
* Check for outliers or inconsistent responses.
* Compute descriptive statistics for each item and block.

---

#### **Step 2: Factor Analysis (Measurement Validation)**

#### 2a: Exploratory Factor Analysis (EFA)

* Purpose: Check whether items cluster as expected.

* Expected clusters:

  1. **Lower-order cognitive engagement**: Remember, Understand, Apply
  2. **Higher-order cognitive engagement**: Analyze, Evaluate, Create
  3. **Learner-oriented / Self-regulated learning**: Reflection, Planning, Innovation, Self-efficacy
  4. **Dependency / Overreliance** (reverse-coded)

* Retain items that load cleanly onto factors (loading > 0.4)

* Check factor reliability (Cronbach’s α ≥ 0.7 per factor)

#### 2b: Confirmatory Factor Analysis (CFA)

* Confirm EFA structure
* Obtain **factor scores** for each respondent (needed for SAMR mapping)
* Validate model fit: CFI, TLI > 0.9; RMSEA < 0.08

---

#### **Step 3: Compute Factor Scores**

* Each respondent gets **continuous factor scores** for:

1. Lower-order cognition
2. Higher-order cognition
3. Agency
4. Dependency / Overreliance 

* These scores summarize the **student’s cognitive and learning behavior profile**.

---

#### **Step 4: Map Configurations to SAMR Levels**

* Use **rule-based or clustering approach**:

#### Option A: Rule-Based

| Pattern                                                      | SAMR Level   |
| ------------------------------------------------------------ | ------------ |
| High lower-order cognition + High dependency, low reflection | Substitution |
| Low Dep + High LOC + High Agency                             | Augmentation |
| High Agency + High HOC + Moderate Dep                        | Modification |
| High Agency + High HOC + Low Dep                             | Redefinition |

* Thresholds for “High” can be empirically set (e.g., top 33% of factor scores).

#### Option B: Clustering

* Use k-means or hierarchical clustering on factor scores
* Identify **clusters of students with similar engagement patterns**
* Map clusters to SAMR levels **post-hoc**, based on cognitive + learner-oriented profiles

---

#### **Step 5: Descriptive & Inferential Analysis**

* **Descriptive:**

  * % of students showing each SAMR-level pattern
  * Average factor scores per task type
  * Tool usage patterns per SAMR level

* **Inferential:**

  * Correlation between learner orientation factors and cognitive engagement
  * Predictive models (optional): which factors predict “Redefinition” patterns
  * Compare subgroups (e.g., undergraduate vs. postgraduate)

---

#### **Step 6: Triangulation with Interview Phase**

* Compare **survey-identified SAMR patterns** with **Phase 1 qualitative findings**
* Examples:

  * Interview: “I use AI to explore new approaches I wouldn’t have tried”
  * Survey: High Create + High Innovation → Redefinition
* Strengthens **construct validity** and shows sequential exploratory logic.

---

#### **Step 7: Interpretation & Reporting**

* Report **factor structure**, reliability, and descriptive statistics first
* Then interpret **SAMR patterns emergent from configurations**
* Include **visualizations**: heatmaps, radar charts, or Sankey diagrams showing Bloom + learner factors → SAMR levels

---

#### **Step 8: Optional Extensions**

* Link SAMR patterns to **learning outcomes or performance measures** (if available)
* Analyze **tool preferences** or frequency as predictors of SAMR-level engagement

---

### Key Points

* **SAMR is never a direct survey factor** — it is **derived post-hoc** from factor score configurations
* Factor analysis ensures your survey is **psychometrically sound**
* This pipeline maintains **sequential exploratory design**:
  Interview → Survey → CFA → Factor scores → Pattern → SAMR interpretation

----

# Connecting the Interview to the Survey 

Together, these analyses determine:

* Whether Phase 1 themes hold statistically
* Which AI practices carry genuine educational value
* Where risks (over-reliance, shallow learning) are concentrated
* How different student populations interact with AI

## 1. How the survey *emerges from the interviews*

Interviews focused on:

* What tasks students use AI for (code explanation, debugging, learning concepts, etc.)
* How they use AI (prompting, reflection, integration into learning)
* Perceived impact (confidence, understanding, speed, engagement)
* Preferences (Socratic tutor vs. answer-giving chatbot)

These discussions will give:

1. **Task categories** → what students actually use AI for
2. **Cognitive processes** → whether they are understanding, applying, analyzing, or creating
3. **Learning behavior patterns** → reflection, reliance, innovation, or overreliance

The **survey blocks** directly map to these insights:

| Interview Insight                                                    | Survey Operationalization                   |
| -------------------------------------------------------------------- | ------------------------------------------- |
| Students use AI to **remember code patterns, syntax, definitions**   | Bloom → Remember                            |
| Students use AI to **understand concepts or code they didn’t write** | Bloom → Understand                          |
| Students use AI to **debug, analyze errors, or improve logic**       | Bloom → Analyze / Evaluate                  |
| Students use AI to **generate new solutions or code creatively**     | Bloom → Create                              |
| Students **reflect on AI’s suggestions or adapt them**               | Learner-oriented → Reflection, Planning     |
| Students **rely on AI without thinking**                             | Learner-oriented → Dependency / Offloading  |
| Students **try new approaches encouraged by AI**                     | Learner-oriented → Innovation / Exploration |

The **interviews give the “content” and patterns**, Bloom + learner-oriented constructs give a **structured measurement tool**.

---

## 2. How to do this rigorously in your thesis

1. **Code interview responses** using thematic analysis:

   * E.g., “AI used to debug” → Code: *Analyze*
   * “AI suggested approach, I tried new things” → Code: *Create / Innovation*
     
2. **Aggregate these codes into themes** that naturally correspond to Bloom levels.
3. **Design survey items to reflect these themes**, preserving participants’ language as much as possible.

This preserves the **emergent nature** of sequential design — survey items are not arbitrarily chosen; they are **operationalizations of qualitative findings**.

---

## 3. Example of Direct Linking

**Interview excerpt:**

> “I usually ask ChatGPT to explain a function I don’t understand. Sometimes it gives me examples, and I can see why it works.”

**Coding:**

* Cognitive: Understand
* Behavior: Reflection on AI explanation

**Survey translation:**

* Bloom: “I use AI to explain concepts or code I don’t fully understand.”
* Learner-oriented: “After using AI, I reflect on what I learned and what I still need to understand.”

**survey item is directly grounded in student words**, preserving qualitative findings.

---

## 4. Why This Strengthens Your Study

* Maintains **exploratory sequential logic**:
  Phase 1 → inductive understanding → Phase 2 → structured measurement
* Avoids **forcing theoretical frameworks** onto students’ experiences
* Ensures **construct validity**: survey measures what students actually do, not what we assume

Essentially doing **thematic translation from interview → survey**: the survey *quantifies the patterns that emerged in Phase 1*.

---

## 5. Adding a “traceability table” in presentation

Include a table in your thesis showing:

| Interview Theme       | Example Quote                                    | Bloom / Learner-Item Mapping | Survey Item ID |
| --------------------- | ------------------------------------------------ | ---------------------------- | -------------- |
| Debugging             | “I ask AI when my code throws an error”          | Analyze                      | A4             |
| Concept Understanding | “AI explains loops I struggle with”              | Understand                   | A2             |
| Creative Use          | “I ask AI to generate new solution approaches”   | Create / Innovation          | A6 / B6        |
| Reflection            | “I check whether the AI explanation makes sense” | Reflection                   | B3             |

This **explicitly demonstrates the survey is grounded in qualitative data**

---

### Takeaway

survey items are **derived directly from them**, translated into **Bloom + learner-oriented constructs** for measurement.
* SAMR remains the **interpretive lens**, not the measurement tool.
* This keeps the **exploratory sequential design intact**: qualitative → quantitative → interpretation.




