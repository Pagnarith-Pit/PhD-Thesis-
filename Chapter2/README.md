# Motivation
Beyond their application as a tutor, since the release of large-scale generative AI models, the higher education sector has been repeatedly identified as one of the domains poised to benefit most profoundly from these technologies. Academic literature has highlighted the potential of generative AI to expand access to personalised feedback, enhance student support, reduce administrative burdens, and provide scalable tutoring capabilities across diverse disciplines. As a result, universities worldwide have faced strong calls from researchers, policymakers, and industry to integrate AI into teaching and learning environments in meaningful ways.

Yet, despite these optimistic projections, a growing counter-movement has emerged, warning against uncritical or extensive adoption of AI in education. Critics argue that generative AI may foster over-reliance, enable academic misconduct, disrupt the development of critical thinking, and reduce opportunities for meaningful peer interaction and engagement within classrooms. Some educators also express concerns about cognitive offloading, arguing that if AI tools are used to bypass effortful thinking—such as solving exercises, generating explanations, or producing writing—students may experience long-term erosion in analytical and problem-solving skills. These opposing narratives have created a polarised discourse in which AI is simultaneously framed as a transformative educational innovation and a potentially harmful shortcut.

If one side of this debate is to ultimately prevail, the true benefits and harms of AI must be empirically established rather than assumed. Surprisingly, however, large-scale institutional adoption remains limited. This is particularly puzzling given that a broad body of non-experimental research—primarily surveys and interviews conducted across diverse geographical regions—reports overwhelmingly positive attitudes toward AI among university students. Students consistently describe AI tools as convenient, accessible, and beneficial for understanding complex content, clarifying concepts, and supporting their studies. These findings might lead one to expect rapid and widespread adoption of AI-driven educational practices.

Instead, studies involving real student usage paint a much more complex and inconsistent picture. In controlled or course-embedded deployments, some disciplines—most notably computer science and programming—report high student engagement with AI assistants and clear preferences for studying with AI support. Students in these fields often describe AI as a “debugging companion,” a “coding partner,” or a “24/7 tutor,” suggesting that AI meaningfully enhances their learning experience. Conversely, other disciplines show low uptake or limited interest in using AI for learning tasks, even when students are familiar with the technology. In some humanities and social science contexts, for example, students express concerns about the accuracy, relevance, or disciplinary appropriateness of AI-generated outputs, leading to cautious or minimal adoption.

Complicating matters further, several recent studies have adopted a more naturalistic, observational approach by analysing students’ real chat logs with AI systems. Unlike controlled experiments, these studies do not restrict the types of questions students can ask or the timeframe of usage. Instead, they capture authentic, everyday interactions. Findings from such analyses reveal that a substantial portion of student queries are not aligned with deep learning goals. Many interactions involve asking AI for direct answers to homework questions, generating assignment-ready content, or using AI for convenience-based tasks rather than meaningful cognitive engagement. Other interactions are unrelated to academic work altogether, involving personal, professional, or entertainment-oriented queries. These observations challenge the assumption that enthusiastic attitudes towards AI necessarily translate into beneficial or pedagogically sound usage patterns.

Together, these contradictions raise a critical question about where the actual educational value of generative AI lies. If student attitudes are overwhelmingly positive, yet real-world usage varies widely and often gravitates toward shallow or non-academic activities, then institutions cannot assume that AI will automatically enhance learning. Understanding the real value of AI is essential not only for justifying its incorporation into everyday university learning environments but also for guiding strategic institutional investment. Moreover, insights into how students genuinely use AI are vital for developers of educational AI systems, who must align their tools with what meaningfully benefits students rather than what is simply convenient or superficially appealing.

These concerns motivate the central problem addressed in this research: although generative AI has rapidly become ubiquitous, its actual impact on students’ learning practices remains unclear. Without a grounded, empirical understanding of how AI reshapes the everyday routines, strategies, and behaviours of university students, the sector risks designing policies, technologies, and teaching practices based on assumptions rather than realities.

## Research Question
Thus, the key research question guiding this study is:

How does the use of generative AI reshape students’ learning practices?

To examine this question, the study further investigates the following subquestions:

1) In what ways do students integrate AI tools with university-provided learning materials (such as textbooks, lecture notes, and LMS content)?

2) What types of learning activities or outcomes are enabled by AI tools, and which are simply eased—made more efficient or convenient—without fundamentally transforming the underlying learning process?

Through addressing these questions, this research aims to illuminate the real, rather than presumed, role of generative AI in contemporary university education and to provide evidence-based guidance for institutions, educators, and developers navigating this rapidly changing landscape.

# Methods

This study adopts a mixed-methods Exploratory Sequential design, integrating qualitative and quantitative research to develop a comprehensive understanding of how university students incorporate generative AI tools into their learning practices. The decision to employ an exploratory design reflects the emerging and understudied nature of day-to-day AI use in higher education. While substantial policy discussions and conceptual debates surround AI in learning, empirical evidence capturing how students actually use AI remains limited. Combining qualitative exploration with quantitative validation therefore enables both openness to new patterns and the ability to evaluate their generalisability across a larger population.

### Justification for Exploratory Design 

Mixed-methods research has been widely adopted in educational technology studies when the aim is to understand both the complexity of human behaviours and the general prevalence of patterns (Creswell & Plano Clark, 2017). Because generative AI is a novel and rapidly evolving tool, student usage has not yet stabilised into predictable or well-defined patterns. Consequently, qualitative inquiry is essential to uncover behaviours, use cases, and perceptions that existing theories or surveys might overlook.

Past research on AI in education has predominantly relied on frameworks such as:

* Technology Acceptance Model (TAM)

* Self-Regulated Learning (SRL)

* Unified Theory of Acceptance and Use of Technology (UTAUT)

* Technology–Task Fit (TTF)

These models have been valuable for assessing perceived usefulness, ease of use, and self-efficacy, yet they offer limited insight into the specific sources of value students derive from AI. In other words, these frameworks explain whether students find AI helpful, but not how or why it changes their learning.

To address this gap, the present study draws on the Substitution, Augmentation, Modification, and Redefinition (SAMR) framework. SAMR provides a structured way to distinguish between:

* Easing uses: where AI substitutes or automates tasks without changing learning practices, and

* Enabling uses: where AI introduces fundamentally new learning opportunities or transforms how tasks are approached.

SAMR is therefore uniquely suited to evaluating whether AI enhances education in a pedagogically meaningful way or merely accelerates or outsources existing tasks. This distinction is especially relevant given current debates on academic integrity, cognitive offloading, and the risk of over-reliance on AI.

## Qualitative Study (Phase 1)
The initial qualitative phase aims to identify the actual ways students integrate generative AI into their everyday coursework. The objective is to generate categories of usage, explore their underlying motivations, and understand whether students experience these practices as transformative or merely convenient.

### Study Design
Data will be collected through semi-structured interviews, each lasting approximately 20 minutes. Semi-structured interviews are chosen because they balance flexibility—allowing students to describe their usage in their own terms—with comparability across participants. This format is particularly appropriate for exploratory technology research, where unknown behaviours may emerge during conversation.

#### Instruments

A custom interview guide will be developed, guided by past SAMR work. Topics will include:

* Concrete examples of recent AI usage within coursework

* Interactions between AI tools and university-provided materials

* Students’ perceived benefits, challenges, and risks

* Perceived changes in study behaviour since adopting AI

* Perceived necessity of AI for certain tasks

The interview guide will be piloted with 2–3 students to ensure clarity and appropriateness.

#### Additional Data to Collect
To contextualise the results and enable later comparison across subgroups, participants will also provide:

* name and email address: only to report findings or contact for any required changes to data. Not used for any analysis
* Year level: to explore whether senior students use AI differently
* course: to account for known differences between STEM and non-STEM use patterns
* self-rated AI familiarity: account for how well students are familiar with their capabilities 
* english level proficiency: studies have reported AI used for translation. Could be indicator here since University of Melbourne is English speaking instritution

#### Recording and Transcription
Interviews will be audio-recorded with participants’ consent. Transcription will be performed using contemporary speech-to-text models known for high accuracy in educational settings. If consent for recording is not granted, detailed notes will be taken instead.

---

### Participants
Participants will be active undergraduate or postgraduate students at the University of Melbourne. Because interviews are conducted in English, participants must demonstrate adequate English proficiency. To ensure relevance, students must have used generative AI at least once in the past six months.

The study will aim for approximately 40 participants or until thematic saturation is reached. This sample size is consistent with exploratory qualitative research where the goal is to map the space of behaviours rather than achieve statistical proportionality.

Recruitment will occur through:

* Flyers posted around campus

* Announcements made in subjects with the support of coordinators

* Personal networks and referrals

All potential participants will receive information about the study’s aims, interview topics, data retention policies, confidentiality protections, result disemmination strategy, and the ethics approval for this project. Participation will proceed only after informed consent is obtained, with sufficient time to review all documents. Participants will be compensated at a pro-rated rate of $50 per hour.

---

### Data Analysis
Two complementary analytical approaches will be used: sentiment analysis and thematic analysis, followed by a computational check using semantic cluster analysis.

#### Sentiment Analysis

Sentiment analysis provides a preliminary quantitative assessment of the emotional tone of participants’ discussions about AI. This is particularly useful given that self-reported attitudes may be influenced by social desirability or normative pressure in higher-education environments. Automated sentiment scoring offers a consistent and unbiased method for identifying whether students’ experiences are broadly positive, negative, or neutral. It also helps detect patterns of enthusiasm, frustration, or ambivalence that might correlate with specific uses of AI.


#### Thematic Analysis

The main analytic process will involve inductive thematic analysis, chosen because it aligns with the exploratory goal of identifying emerging patterns without forcing responses into predefined categories.

Procedure: 

1) Initial coding

Each interview transcript will be read closely, and segments describing specific AI-related behaviours will be given short descriptive codes (e.g., "summarising lectures", "debugging code", "checking grammar"). Coding will be iterative and evolve as new patterns emerge.

2) Code refinement and grouping

Related codes will be examined and clustered into broader categories or subthemes. These may include categories like “efficiency-driven use,” “task outsourcing,” “conceptual clarification,” or “creativity support.” (Examples only; actual themes will emerge from data.)

3) Theme generation

The subthemes will then be grouped into overarching themes that capture core dimensions of how students apply AI in their study routines.

Because the coding will be conducted by a single primary researcher, the study incorporates an additional computational validity check.


#### Sematic Cluster Analysis (Bias Mitigation)

To reduce potential coder bias and strengthen the reliability of the thematic structure, the study will use semantic cluster analysis:

* Each individual usage example (coded excerpt) will be embedded using a modern text-embedding model.

* A centroid will be computed for each theme.

* Distances between each usage example and all theme centroids will be calculated.

* Instances where the assigned theme is not the nearest centroid will be flagged for review.

This method, commonly used in topic modelling and clustering research, allows a systematic, data-driven check on whether usage descriptions are grouped coherently.

#### Enabling vs. Easing Coding

After identifying themes, each will be evaluated through the lens of the SAMR framework:

* Enabling (Modification/Redefinition):
AI is used in a way that introduces new learning behaviours or capabilities not previously possible. These reflect AI’s potential as a transformative educational technology.

* Easing (Substitution/Augmentation):
AI is used to speed up, automate, or refine existing tasks without fundamentally changing the form of the learning activity. These uses suggest AI may be helpful but not pedagogically significant.

This stage is essential for interpreting not just how students use AI, but whether those uses align with meaningful learning enhancement. It also directly contributes to ongoing debates about whether AI’s benefits outweigh its risks, particularly when uses lean toward convenience or task outsourcing rather than conceptual development.

## Quantitative Study (Phase 2)
Building on the qualitative insights generated in Phase 1, Phase 2 aims to evaluate the generalisability, prevalence, and educational significance of students’ AI usage patterns across a larger and more diverse student population. While Phase 1 identifies how students use AI and why they perceive certain benefits, Phase 2 examines how widespread these behaviours are, how they vary across demographic and academic groups, and whether different forms of AI use (enabling vs. easing) are associated with meaningful learning outcomes. This phase serves to validate the qualitative findings, quantify their distribution, and provide empirical evidence to guide institutional policy and pedagogical decisions.

Phase 2 transforms the rich qualitative insights from Phase 1 into quantifiable, testable constructs, enabling:

* Large-scale validation of usage patterns
* Identification of population-wide trends
* Insights into enabling vs. easing practices
* Evidence-based recommendations for university AI policy
* Clear implications for designing future AI-based educational tools

## **Participants**

The target population mirrors that of Phase 1 but expands its scope to achieve statistical power and representativeness.

### **Who**

Participants will include:

* Undergraduate and graduate students enrolled at the University of Melbourne.
* Students from all faculties (STEM and non-STEM) to capture disciplinary variation in AI use.
* Students who have used generative AI tools (e.g., ChatGPT, Claude, Gemini, GitHub Copilot) within the past six months, ensuring responses reflect current habits and not outdated behaviours.

No restrictions will be placed on age, residency status, or academic standing.

### **Sample Size**

The study aims to recruit **at least 300 participants**.
This sample size is chosen to:

* Enable stable estimation for multivariate analyses (e.g., regression, cluster analysis).
* Provide statistical confidence when comparing subgroups (e.g., STEM vs non-STEM; novice vs expert AI users).
* Ensure adequate representation of diverse AI usage patterns identified in Phase 1.

### **Recruitment**

Recruitment will occur through multiple channels to ensure broad reach:

* Learning Management System (LMS) announcements with support from course coordinators.
* Flyers posted in student spaces.
* Email invitations via faculty mailing lists where permissible.
* Social media posts within university-affiliated groups.
* Snowball sampling, where participants may share the study link with peers.

Participants will receive detailed information about the study’s aims, data handling procedures, risks, and consent requirements prior to participation.

---

## **Study Design**

Phase 2 uses a cross-sectional online survey, administered via Google Forms, allowing efficient distribution to a large number of students and enabling data collection within a consistent and structured format.

### **Instrument**

A survey instrument will be developed based on:

1. **Themes and usage types identified in Phase 1 thematic analysis**, including:

   * Concept explanation
   * Assignment/task support
   * Brainstorming
   * Code debugging or walkthroughs
   * Study planning and organisation
   * Translation or language clarification
   * Personal or non-academic use

2. **SAMR-aligned enabling/easing classifications** derived from Phase 1.

3. **Sentiment patterns** identified earlier (e.g., positive, negative, neutral orientation towards AI).

4. **Established constructs** from learning sciences and technology adoption literature, such as:

   * Perceived usefulness
   * Self-regulated learning behaviours
   * Cognitive offloading tendencies
   * Perceived academic integrity risks
   * AI literacy or proficiency

Items will be primarily Likert scale (1–5), with some categorical and open-ended fields to capture nuanced responses.

### **Data Collected**

The survey will collect multiple categories of data to support rich quantitative analysis:

---

### **1. Demographic and Academic Background**

* Faculty/discipline (STEM, humanities, commerce, health sciences, etc.)
* Level of study (undergraduate, graduate, PhD)
* Year level
* Domestic vs. international status
* English language proficiency (self-rated)
* Prior academic performance (self-reported average grade band)

Purpose: To examine how AI usage differs across student groups and educational contexts.

---

### **2. AI Familiarity and Usage Profile**

* Frequency of AI use (daily, weekly, monthly)
* Duration of AI use (in months/years)
* Self-rated AI proficiency
* AI tools used (ChatGPT, Gemini, Claude, Copilot, Perplexity, others)

Purpose: Controls for familiarity and exposure.

---

### **3. AI Usage Behaviours (Based on Phase 1 Themes)**

Participants will rate how often they use AI for each usage category identified qualitatively, e.g.:

* Explain difficult concepts
* Summarise lectures or textbooks
* Translate or simplify text
* Generate ideas or brainstorm
* Assist with assignments
* Debug programming tasks
* Review writing
* Organise study plans
* Provide emotional or motivational support
* Non-academic personal tasks

Purpose: To quantify usage prevalence and identify usage clusters.

---

### **4. SAMR-Aligned Enabling vs. Easing Classification**

For each usage activity, students will be asked:

* Whether AI allowed them to perform a task they *could not do before* (enabling/transformation)
* Whether AI primarily made the task *faster or easier* (easing/substitution)

Purpose: To measure the degree of pedagogical transformation and evaluate the educational significance of AI use.

---

### **5. Perceived Learning Outcomes**

Students will rate:

* Perceived understanding of concepts when using AI
* Confidence in completing tasks
* Engagement with course materials
* Self-reported academic performance changes
* Over-reliance concerns
* Perceived risks to academic integrity

Purpose: To link AI use to meaningful learning indicators.

---

### **6. Attitudes, Sentiments, and Emotional Orientation**

Based on Phase 1 sentiment patterns, students will respond to items capturing:

* Trust in AI tools
* Enjoyment or frustration when using AI
* Anxiety about misuse or errors
* Overall positivity or negativity toward AI

Purpose: To understand emotional drivers of behaviour.

---

### **7. Open-Ended Questions**

A small number of optional open-ended questions will capture nuance, e.g.:

* “Describe a time AI significantly improved your learning.”
* “Describe a time AI negatively affected your learning or engagement.”

---

## **Analysis Plan**

Phase 2 will use a combination of descriptive, inferential, and exploratory statistical analyses:

### **Descriptive Statistics**

* Prevalence of each usage pattern
* Frequency distributions
* Mean scores for enabling vs. easing behaviours

### **Inferential Tests**

* Comparisons between groups (ANOVA, t-tests)
* Correlations between AI proficiency, usage types, and outcomes
* Regressions predicting learning outcomes from usage categories

### **Structural and Exploratory Analyses**

* **Factor analysis** to validate thematic structures emergent in Phase 1
* **Cluster analysis** to identify student archetypes (e.g., "Transformative Users", "Efficiency Seekers", "Answer Dependents", "Minimal Users")
* **Path models** exploring how AI proficiency & usage mediate learning outcomes

Together, these analyses determine:

* Whether Phase 1 themes hold statistically
* Which AI practices carry genuine educational value
* Where risks (over-reliance, shallow learning) are concentrated
* How different student populations interact with AI



